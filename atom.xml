<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>刘阳的个人博客</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://lylllcc.github.io/"/>
  <updated>2017-05-01T15:33:20.842Z</updated>
  <id>https://lylllcc.github.io/</id>
  
  <author>
    <name>lylllcc</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>图片隐藏信息</title>
    <link href="https://lylllcc.github.io/2017/05/01/%E5%9B%BE%E7%89%87%E9%9A%90%E8%97%8F%E4%BF%A1%E6%81%AF/"/>
    <id>https://lylllcc.github.io/2017/05/01/图片隐藏信息/</id>
    <published>2017-05-01T15:22:12.000Z</published>
    <updated>2017-05-01T15:33:20.842Z</updated>
    
    <content type="html"><![CDATA[<p>图片的像素里存储着RGB的信息。R、G、B分别为该像素的红、绿、蓝通道，每个通道的分量值范围在0~255，16进制则是00~FF，RGB分量值的小量变动，是肉眼无法分辨的，不影响对图片的识别。利用这个特性完全可以用算法对图片写入数据。</p>
<p>熄灯了，我也懒得写了，直接放代码吧。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;canvas id=&quot;canvas&quot; width=&quot;256&quot; height=&quot;256&quot;&gt;&lt;/canvas&gt;</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line">var ctx = document.getElementById(&apos;canvas&apos;).getContext(&apos;2d&apos;);</div><div class="line">var textData;</div><div class="line">ctx.font = &apos;30px Microsoft Yahei&apos;;</div><div class="line">ctx.fillText(&apos;我只想被温柔的对待&apos;, 60, 130);</div><div class="line">textData = ctx.getImageData(0, 0, ctx.canvas.width, ctx.canvas.height).data;</div><div class="line">var img = new Image();</div><div class="line">var originalData;</div><div class="line">img.onload = function() &#123;</div><div class="line">    ctx.drawImage(img, 0, 0);</div><div class="line">    originalData = ctx.getImageData(0,0,ctx.canvas.width,ctx.canvas.height);</div><div class="line">    processData(originalData);  //加密</div><div class="line">   //mergeData(textData,&apos;R&apos;);   //解密</div><div class="line">&#125;;</div><div class="line">img.src = &apos;2.png&apos;;</div><div class="line"></div><div class="line">var processData = function(originalData)&#123;</div><div class="line">    var data = originalData.data;</div><div class="line">    for(var i = 0; i &lt; data.length; i++)&#123;</div><div class="line">        if(i % 4 == 0)&#123;</div><div class="line">            if(data[i] % 2 == 0)&#123;</div><div class="line">                data[i] = 0;</div><div class="line">            &#125; else &#123;</div><div class="line">                data[i] = 255;</div><div class="line">            &#125;</div><div class="line">        &#125; else if(i % 4 == 3)&#123;</div><div class="line">            continue;</div><div class="line">        &#125; else &#123;</div><div class="line">            data[i] = 0;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    ctx.putImageData(originalData, 0, 0);</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">var mergeData = function(newData, color)&#123;</div><div class="line">    var oData = originalData.data;</div><div class="line">    var bit, offset;</div><div class="line">    switch(color)&#123;</div><div class="line">        case &apos;R&apos;:</div><div class="line">            bit = 0;</div><div class="line">            offset = 3;</div><div class="line">            break;</div><div class="line">        case &apos;G&apos;:</div><div class="line">            bit = 1;</div><div class="line">            offset = 2;</div><div class="line">            break;</div><div class="line">        case &apos;B&apos;:</div><div class="line">            bit = 2;</div><div class="line">            offset = 1;</div><div class="line">            break;</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    for(var i = 0; i &lt; oData.length; i++)&#123;</div><div class="line">        if(i % 4 == bit)&#123;</div><div class="line">            if(newData[i + offset] === 0 &amp;&amp; (oData[i] % 2 === 1))&#123;</div><div class="line">                if(oData[i] === 255)&#123;</div><div class="line">                    oData[i]--;</div><div class="line">                &#125; else &#123;</div><div class="line">                    oData[i]++;</div><div class="line">                &#125;</div><div class="line">            &#125; else if (newData[i + offset] !== 0 &amp;&amp; (oData[i] % 2 === 0))&#123;</div><div class="line">                if(oData[i] === 255)&#123;</div><div class="line">                    oData[i]--;</div><div class="line">                &#125; else &#123;</div><div class="line">                    oData[i]++;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    ctx.putImageData(originalData, 0, 0);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;图片的像素里存储着RGB的信息。R、G、B分别为该像素的红、绿、蓝通道，每个通道的分量值范围在0~255，16进制则是00~FF，RGB分量值的小量变动，是肉眼无法分辨的，不影响对图片的识别。利用这个特性完全可以用算法对图片写入数据。&lt;/p&gt;
&lt;p&gt;熄灯了，我也懒得写了，直
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>使用腾讯云服务器做ss转发</title>
    <link href="https://lylllcc.github.io/2017/04/30/%E4%BD%BF%E7%94%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%81%9Ass%E8%BD%AC%E5%8F%91/"/>
    <id>https://lylllcc.github.io/2017/04/30/使用腾讯云服务器做ss转发/</id>
    <published>2017-04-30T02:33:20.000Z</published>
    <updated>2017-04-30T03:12:57.194Z</updated>
    
    <content type="html"><![CDATA[<p>刚开始用的是版瓦工的服务器，后来因为一些原因账号不能用了，如果还想用的话，还要再申请账号，还要再买，虽然版瓦工很便宜，但是最近手头相当紧呀，也不想再太折腾，于是就买了个70块钱一年的ss账号。链接我就不放了，谷歌搜一下ss账号第一个就是。<br>这个账号不限流量，但是限制连接的个数（其实我觉得没限制），为了方便广大吃瓜群众科学上网，我用腾讯云做了一次转发，让大家连在我的腾讯云上，然后腾讯云连这个ss。<br>实现方法有好多，但好多我也都失败了，最后我是参考<a href="https://doub.io/ss-jc29/" target="_blank" rel="external">这个</a>成功的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;刚开始用的是版瓦工的服务器，后来因为一些原因账号不能用了，如果还想用的话，还要再申请账号，还要再买，虽然版瓦工很便宜，但是最近手头相当紧呀，也不想再太折腾，于是就买了个70块钱一年的ss账号。链接我就不放了，谷歌搜一下ss账号第一个就是。&lt;br&gt;这个账号不限流量，但是限制连
    
    </summary>
    
    
      <category term="linux" scheme="https://lylllcc.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>c++中delete和delete[]的区别</title>
    <link href="https://lylllcc.github.io/2017/04/12/c++%E4%B8%ADdelete%E5%92%8Cdelete%5B%5D%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>https://lylllcc.github.io/2017/04/12/c++中delete和delete[]的区别/</id>
    <published>2017-04-12T14:33:48.000Z</published>
    <updated>2017-04-13T22:34:27.222Z</updated>
    
    <content type="html"><![CDATA[<p>今天做了个改错题，跪在析构函数的delete上了。回来查了一下delete的具体用法，整理一下。<br>网上说 delete[] 是释放当初new的整个数组，而delete 释放的数组的第一个元素。写了个简单的程序测试了一下。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">int *a = new int[10];</div><div class="line">//delete[] a;</div><div class="line">delete a;</div><div class="line"></div><div class="line">return 0;</div></pre></td></tr></table></figure></p>
<p> 单步调试的时候我发现 <img src="/imgs/2017412/1.png" alt="1"> 运行delete a;那一行之后全部开的内存都被释放掉了。<br> 查了好久，原来c++的delete是调用对象的析构函数来实现的。而基本的数据类型没有析构函数，在delete的时候直接全部释放掉了。<br> 于是自己写了一个来测试一下，自定义数据类型是否满足。<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"> #include &lt;iostream&gt;</div><div class="line"></div><div class="line">using namespace std;</div><div class="line">class A&#123;</div><div class="line">public:</div><div class="line">    int *b;</div><div class="line">    A()</div><div class="line">    &#123;</div><div class="line">        b = new int[10];</div><div class="line">        b[0] = 1;</div><div class="line">    &#125;</div><div class="line">    ~A()</div><div class="line">    &#123;</div><div class="line">        delete[] b;</div><div class="line">    &#125;</div><div class="line">&#125;;</div><div class="line"></div><div class="line">int main()</div><div class="line">&#123;</div><div class="line">    int *a = new int[10];</div><div class="line">    //delete[] a;</div><div class="line">    delete a;</div><div class="line">    cout&lt;&lt;a[1].b[0];</div><div class="line"></div><div class="line">    return 0;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>我在自己定义的类A中定义了一个数组，然后在构造函数中给数组的第一个元素赋值。然后delete a;如果上述成立的话，那么应该只有 a[0]被释放了内存，而a[1]这个对象还在内存中，事实证明，他果然还在。</p>
<p>所以 如果是 new 一个对象的话，可以用delete来释放内存 如果是new 一个数组的话，就要delete[] 才能清除干净。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天做了个改错题，跪在析构函数的delete上了。回来查了一下delete的具体用法，整理一下。&lt;br&gt;网上说 delete[] 是释放当初new的整个数组，而delete 释放的数组的第一个元素。写了个简单的程序测试了一下。&lt;br&gt;&lt;figure class=&quot;highl
    
    </summary>
    
      <category term="c++" scheme="https://lylllcc.github.io/categories/c/"/>
    
    
  </entry>
  
  <entry>
    <title>用scrapy抓取笔趣阁上所有小说</title>
    <link href="https://lylllcc.github.io/2017/01/15/%E7%94%A8scrapy%E6%8A%93%E5%8F%96%E7%AC%94%E8%B6%A3%E9%98%81%E4%B8%8A%E6%89%80%E6%9C%89%E5%B0%8F%E8%AF%B4/"/>
    <id>https://lylllcc.github.io/2017/01/15/用scrapy抓取笔趣阁上所有小说/</id>
    <published>2017-01-15T14:30:51.000Z</published>
    <updated>2017-03-20T03:23:07.719Z</updated>
    
    <content type="html"><![CDATA[<p>之前也没有认真写过爬虫项目，这应该算我第一个比较正式的爬虫项目。主要的目的就是练习一下<a href="https://scrapy.org/" target="_blank" rel="external">scrapy</a>框架。一个高效强大的爬虫框架。文档在<a href="https://doc.scrapy.org/en/0.24/" target="_blank" rel="external">这里</a></p>
<p>这个爬虫的目的是抓取<a href="http://www.qu.la/" target="_blank" rel="external">笔趣阁</a>(因为是盗版网站，链接可能失效)上所有的小说。</p>
<p>首先安装scrapy<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install scrapy</div></pre></td></tr></table></figure></p>
<p>我使用的是windows，python版本是3.5，在安装的时候报错，因为缺少vc的支持库，不能编译c的代码，可以通过去微软官方下载<a href="http://landinghub.visualstudio.com/visual-cpp-build-tools" target="_blank" rel="external">vc++ build tool</a>解决。</p>
<p>创建项目<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy startproject biquge</div></pre></td></tr></table></figure></p>
<p>成功之后的目录结构:<br><img src="/imgs/2017115/1.png" alt="目录"></p>
<p>因为scrapy默认是不能在cmd下进行调试的，我们建立一个入口文件，使他能够在cmd中调试，在工程根目录下建立start.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">from scrapy.cmdline import execute</div><div class="line">execute([&apos;scrapy&apos;, &apos;crawl&apos;, &apos;biquge&apos;])</div></pre></td></tr></table></figure></p>
<p>为了减轻网站的压力，也为了调试迅速，启动缓存。去掉setting.py中最后几行的注释<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">HTTPCACHE_ENABLED = True</div><div class="line">HTTPCACHE_EXPIRATION_SECS = 0</div><div class="line">HTTPCACHE_DIR = &apos;httpcache&apos;</div><div class="line">HTTPCACHE_IGNORE_HTTP_CODES = []</div><div class="line">HTTPCACHE_STORAGE = &apos;scrapy.extensions.httpcache.FilesystemCacheStorage&apos;</div></pre></td></tr></table></figure></p>
<p>准备工作就绪，下面开始写爬虫程序。<br>首先抓取基本的信息，小说的名字，id，作者。</p>
<h4 id="定义字段"><a href="#定义字段" class="headerlink" title="定义字段"></a>定义字段</h4><p>在item.py中写入下面的代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">class BiqugeItem(scrapy.Item):</div><div class="line"></div><div class="line">    # id</div><div class="line">    novel_id = scrapy.Field()</div><div class="line">    # 小说名字</div><div class="line">    name = scrapy.Field()</div><div class="line">    # 小说作者</div><div class="line">    author = scrapy.Field()</div><div class="line">    # 小说url</div><div class="line">    url = scrapy.Field()</div><div class="line">    # 小说简介</div><div class="line">    content = scrapy.Field()</div></pre></td></tr></table></figure></p>
<h4 id="编写spider"><a href="#编写spider" class="headerlink" title="编写spider"></a>编写spider</h4><p>在spiders文件夹下新建，biqugespider.py，定义爬虫入口<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line">from scrapy.http import Request</div><div class="line">from biquge.items import BiqugeItem</div><div class="line">import re</div><div class="line"></div><div class="line">class BiqugeSpider(scrapy.Spider):</div><div class="line">    # 爬虫的名字，和start.py中相对应。</div><div class="line">    name = &apos;biquge&apos;</div><div class="line">    </div><div class="line">    base_url = &apos;http://www.qu.la&apos;</div><div class="line"></div><div class="line">    # 爬虫入口</div><div class="line">    def start_requests(self):</div><div class="line">        start_url = self.base_url + &apos;/xiaoshuodaquan/&apos;</div><div class="line">        yield Request(start_url, callback=self.get_novel_url)</div></pre></td></tr></table></figure></p>
<p>scrapy的Request()会把response自动返回callback的函数。<br>定义 get_novel_url<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># 获取小说url</div><div class="line">def get_novel_url(self, response):</div><div class="line">    base_a = response.xpath(&apos;//div[@class=&quot;novellist&quot;]/ul/li/a/@href&apos;).extract()</div><div class="line">    for a in base_a:</div><div class="line">        novel_a = self.base_url + a</div><div class="line">        yield Request(novel_a, callback=self.get_information_and_chapter, meta=&#123;&quot;novel_a&quot;: novel_a&#125;)</div></pre></td></tr></table></figure></p>
<p>可以使用meta参数进行传值，把meta中的结果传到，回掉函数的response中，使用response.meta[‘name’]取值。</p>
<p>接下来获取小说的基本信息。并存在item中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"># 获取基本信息</div><div class="line">def get_information_and_chapter(self, response):</div><div class="line">    item = BiqugeItem()</div><div class="line">    item[&apos;content&apos;] = &apos;&apos;.join(response.xpath(&apos;//meta[@property=&quot;og:description&quot;]/@content&apos;).extract()). \</div><div class="line">        replace(&apos; &apos;, &apos;&apos;). \</div><div class="line">        replace(&apos;\n&apos;, &apos;&apos;)</div><div class="line"></div><div class="line">    # 保存小说链接</div><div class="line">    novel_url = response.meta[&apos;novel_a&apos;]</div><div class="line">    item[&apos;url&apos;] = novel_url</div><div class="line"></div><div class="line">    # 提取小说名字</div><div class="line">    novel_name = &apos;&apos;.join(response.xpath(&apos;//meta[@property=&quot;og:novel:book_name&quot;]/@content&apos;).extract())</div><div class="line">    item[&apos;name&apos;] = novel_name</div><div class="line"></div><div class="line">    # 提取小说作者</div><div class="line">    item[&apos;author&apos;] = &apos;&apos;.join(response.xpath(&apos;//meta[@property=&quot;og:novel:author&quot;]/@content&apos;).extract())</div><div class="line"></div><div class="line">    # 从url中提取小说id</div><div class="line">    novel_id = &apos;&apos;.join(re.findall(&apos;\d&apos;, novel_url))</div><div class="line">    item[&apos;novel_id&apos;] = novel_id</div><div class="line">    yield item</div></pre></td></tr></table></figure></p>
<p>我使用mysql来进行存取数据，先建一张表，存小说的基本信息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE `novel`.`biquge_information` ( `id` INT NOT NULL AUTO_INCREMENT , `novel_id` VARCHAR(255) NOT NULL , `author` VARCHAR(255) NOT NULL , `url` VARCHAR(255) NOT NULL , `novel_name` VARCHAR(255) NOT NULL , `content` VARCHAR(255) NOT NULL , PRIMARY KEY (`id`)) ENGINE = InnoDB;</div></pre></td></tr></table></figure>
<p>在目录下新建一个包mysqlpipelines 在下面新建 <strong>init</strong>.py pipelines.py  sql.py<br><img src="/imgs/2017115/1.png" alt="结构"></p>
<p>需要安装mysql-connector<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install mysql-connector</div></pre></td></tr></table></figure></p>
<p>在setting.py中设置mysql的基本信息。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">MYSQL_HOST = &apos;127.0.0.1&apos;</div><div class="line">MYSQL_USER = &apos;root&apos;</div><div class="line">MYSQL_PASSWORD = &apos;&apos;</div><div class="line">MYSQL_PORT = &apos;3306&apos;</div><div class="line">MYSQL_DB = &apos;novel&apos;</div></pre></td></tr></table></figure></p>
<p>初始一个mysql游标，并且定义两个方法，其中一个用于插入数据，另外一个用于判断数据是否存在。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">import mysql.connector</div><div class="line">from biquge import settings</div><div class="line"></div><div class="line">cnx = mysql.connector.connect(user=settings.MYSQL_USER, password=settings.MYSQL_PASSWORD,</div><div class="line">                              host=settings.MYSQL_HOST, database=settings.MYSQL_DB)</div><div class="line">cur = cnx.cursor(buffered=True)</div><div class="line"></div><div class="line"></div><div class="line">class Sql:</div><div class="line">    @classmethod</div><div class="line">    def insert_infor(cls, novel_id, author, name, url, content):</div><div class="line">        sql = &apos;INSERT INTO `biquge_information` (`novel_id`, `novel_name`, `author`, `url`, `content`) &apos; \</div><div class="line">              &apos;VALUES (%(novel_id)s, %(novel_name)s, %(author)s, %(url)s, %(content)s)&apos;</div><div class="line">        value = &#123;</div><div class="line">            &apos;novel_id&apos;: novel_id,</div><div class="line">            &apos;novel_name&apos;: name,</div><div class="line">            &apos;author&apos;: author,</div><div class="line">            &apos;url&apos;: url,</div><div class="line">            &apos;content&apos;: content</div><div class="line">        &#125;</div><div class="line">        cur.execute(sql, value)</div><div class="line">        cnx.commit()</div><div class="line"></div><div class="line">    @classmethod</div><div class="line">    def select_novel_id(cls, novel_id):</div><div class="line">        sql = &apos;SELECT EXISTS(SELECT 1 FROM biquge_information WHERE novel_id=%(novel_id)s)&apos;</div><div class="line">        value = &#123;</div><div class="line">            &apos;novel_id&apos;: novel_id</div><div class="line">        &#125;</div><div class="line">        cur.execute(sql, value)</div><div class="line">        return cur.fetchall()[0]</div></pre></td></tr></table></figure>
<p>编写pipelines.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">from .sql import Sql</div><div class="line">from biquge.items import BiqugeItem</div><div class="line">from biquge.items import ChapterContentItem</div><div class="line"></div><div class="line"></div><div class="line">class BiqugePipeline(object):</div><div class="line"></div><div class="line">    def process_item(self, item, spider):</div><div class="line"></div><div class="line">        if isinstance(item, BiqugeItem):</div><div class="line">            novel_id = item[&apos;novel_id&apos;]</div><div class="line">            ret = Sql.select_novel_id(novel_id)</div><div class="line">            if ret[0] == 1:</div><div class="line">                print(&apos;小说已经存在&apos;)</div><div class="line">                pass</div><div class="line">            else:</div><div class="line">                name = item[&apos;name&apos;]</div><div class="line">                author = item[&apos;author&apos;]</div><div class="line">                content = item[&apos;content&apos;]</div><div class="line">                url = item[&apos;url&apos;]</div><div class="line">                print(&apos;小说不存在&apos;)</div><div class="line">                Sql.insert_infor(novel_id, author, name, url, content)</div></pre></td></tr></table></figure>
<p>运行start.py，爬虫就可以抓取笔趣阁上所有小说的基本信息了。<br>当然也可以抓取小说章节内容，可是懒癌犯了，就不写了，详见<a href="https://github.com/lylllcc/biquge.git" target="_blank" rel="external">完整项目</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前也没有认真写过爬虫项目，这应该算我第一个比较正式的爬虫项目。主要的目的就是练习一下&lt;a href=&quot;https://scrapy.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;scrapy&lt;/a&gt;框架。一个高效强大的爬虫框架。文档在&lt;a hr
    
    </summary>
    
    
      <category term="python" scheme="https://lylllcc.github.io/tags/python/"/>
    
      <category term="爬虫" scheme="https://lylllcc.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="scrapy" scheme="https://lylllcc.github.io/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>让我帮你百度一下</title>
    <link href="https://lylllcc.github.io/2015/12/04/%E8%AE%A9%E6%88%91%E5%B8%AE%E4%BD%A0%E7%99%BE%E5%BA%A6%E4%B8%80%E4%B8%8B/"/>
    <id>https://lylllcc.github.io/2015/12/04/让我帮你百度一下/</id>
    <published>2015-12-03T22:35:30.000Z</published>
    <updated>2017-04-13T22:49:53.920Z</updated>
    
    <content type="html"><![CDATA[<p>今天在群里看到一个链接，点开之后进入一个仿百度的界面，界面自动播放动画，自动把内容输入到搜索框，然后自动搜索。感觉挺有趣的，正好最近在看jquery，于是我就用jquery实现了一下这个功能。<br>项目演示地址 <a href="http://lylllcc.github.io/project/help/">让我帮你百度一下</a><br>用法很简单，只要你把想搜的东西放到搜索框里，然后点生成链接，就可以把链接发给小伙伴去捉弄一下她们了。<br>实现过程也很简单，生成的那个链接其实还是会跑到我们的项目演示地址来，url中有要搜索的参数，用js把要搜索的参数提取下来，让动画播放，然后把要搜索的参数post到百度就可以了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天在群里看到一个链接，点开之后进入一个仿百度的界面，界面自动播放动画，自动把内容输入到搜索框，然后自动搜索。感觉挺有趣的，正好最近在看jquery，于是我就用jquery实现了一下这个功能。&lt;br&gt;项目演示地址 &lt;a href=&quot;http://lylllcc.github
    
    </summary>
    
      <category term="boring" scheme="https://lylllcc.github.io/categories/boring/"/>
    
    
  </entry>
  
</feed>
