<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>刘阳的个人博客</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://lylllcc.github.io/"/>
  <updated>2017-04-12T14:47:22.582Z</updated>
  <id>https://lylllcc.github.io/</id>
  
  <author>
    <name>lylllcc</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>c++中delete和delete[]的区别</title>
    <link href="https://lylllcc.github.io/2017/04/12/c++%E4%B8%ADdelete%E5%92%8Cdelete%5B%5D%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>https://lylllcc.github.io/2017/04/12/c++中delete和delete[]的区别/</id>
    <published>2017-04-12T14:33:48.000Z</published>
    <updated>2017-04-12T14:47:22.582Z</updated>
    
    <content type="html"><![CDATA[<p>今天做了个改错题，跪在析构函数的delete上了。回来查了一下delete的具体用法，整理一下。<br>网上说 delete[] 是释放当初new的整个数组，而delete 释放的数组的第一个元素。写了个简单的程序测试了一下。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">int *a = new int[10];</div><div class="line">//delete[] a;</div><div class="line">delete a;</div><div class="line"></div><div class="line">return 0;</div></pre></td></tr></table></figure></p>
<p> 单步调试的时候我发现 <img src="imgs/2017/4/12" alt="1"> 运行delete a;那一行之后全部开的内存都被释放掉了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天做了个改错题，跪在析构函数的delete上了。回来查了一下delete的具体用法，整理一下。&lt;br&gt;网上说 delete[] 是释放当初new的整个数组，而delete 释放的数组的第一个元素。写了个简单的程序测试了一下。&lt;br&gt;&lt;figure class=&quot;highl
    
    </summary>
    
      <category term="c++" scheme="https://lylllcc.github.io/categories/c/"/>
    
    
  </entry>
  
  <entry>
    <title>用scrapy抓取笔趣阁上所有小说</title>
    <link href="https://lylllcc.github.io/2017/01/15/%E7%94%A8scrapy%E6%8A%93%E5%8F%96%E7%AC%94%E8%B6%A3%E9%98%81%E4%B8%8A%E6%89%80%E6%9C%89%E5%B0%8F%E8%AF%B4/"/>
    <id>https://lylllcc.github.io/2017/01/15/用scrapy抓取笔趣阁上所有小说/</id>
    <published>2017-01-15T14:30:51.000Z</published>
    <updated>2017-03-20T03:23:07.719Z</updated>
    
    <content type="html"><![CDATA[<p>之前也没有认真写过爬虫项目，这应该算我第一个比较正式的爬虫项目。主要的目的就是练习一下<a href="https://scrapy.org/" target="_blank" rel="external">scrapy</a>框架。一个高效强大的爬虫框架。文档在<a href="https://doc.scrapy.org/en/0.24/" target="_blank" rel="external">这里</a></p>
<p>这个爬虫的目的是抓取<a href="http://www.qu.la/" target="_blank" rel="external">笔趣阁</a>(因为是盗版网站，链接可能失效)上所有的小说。</p>
<p>首先安装scrapy<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install scrapy</div></pre></td></tr></table></figure></p>
<p>我使用的是windows，python版本是3.5，在安装的时候报错，因为缺少vc的支持库，不能编译c的代码，可以通过去微软官方下载<a href="http://landinghub.visualstudio.com/visual-cpp-build-tools" target="_blank" rel="external">vc++ build tool</a>解决。</p>
<p>创建项目<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy startproject biquge</div></pre></td></tr></table></figure></p>
<p>成功之后的目录结构:<br><img src="/imgs/2017115/1.png" alt="目录"></p>
<p>因为scrapy默认是不能在cmd下进行调试的，我们建立一个入口文件，使他能够在cmd中调试，在工程根目录下建立start.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">from scrapy.cmdline import execute</div><div class="line">execute([&apos;scrapy&apos;, &apos;crawl&apos;, &apos;biquge&apos;])</div></pre></td></tr></table></figure></p>
<p>为了减轻网站的压力，也为了调试迅速，启动缓存。去掉setting.py中最后几行的注释<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">HTTPCACHE_ENABLED = True</div><div class="line">HTTPCACHE_EXPIRATION_SECS = 0</div><div class="line">HTTPCACHE_DIR = &apos;httpcache&apos;</div><div class="line">HTTPCACHE_IGNORE_HTTP_CODES = []</div><div class="line">HTTPCACHE_STORAGE = &apos;scrapy.extensions.httpcache.FilesystemCacheStorage&apos;</div></pre></td></tr></table></figure></p>
<p>准备工作就绪，下面开始写爬虫程序。<br>首先抓取基本的信息，小说的名字，id，作者。</p>
<h4 id="定义字段"><a href="#定义字段" class="headerlink" title="定义字段"></a>定义字段</h4><p>在item.py中写入下面的代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">class BiqugeItem(scrapy.Item):</div><div class="line"></div><div class="line">    # id</div><div class="line">    novel_id = scrapy.Field()</div><div class="line">    # 小说名字</div><div class="line">    name = scrapy.Field()</div><div class="line">    # 小说作者</div><div class="line">    author = scrapy.Field()</div><div class="line">    # 小说url</div><div class="line">    url = scrapy.Field()</div><div class="line">    # 小说简介</div><div class="line">    content = scrapy.Field()</div></pre></td></tr></table></figure></p>
<h4 id="编写spider"><a href="#编写spider" class="headerlink" title="编写spider"></a>编写spider</h4><p>在spiders文件夹下新建，biqugespider.py，定义爬虫入口<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line">from scrapy.http import Request</div><div class="line">from biquge.items import BiqugeItem</div><div class="line">import re</div><div class="line"></div><div class="line">class BiqugeSpider(scrapy.Spider):</div><div class="line">    # 爬虫的名字，和start.py中相对应。</div><div class="line">    name = &apos;biquge&apos;</div><div class="line">    </div><div class="line">    base_url = &apos;http://www.qu.la&apos;</div><div class="line"></div><div class="line">    # 爬虫入口</div><div class="line">    def start_requests(self):</div><div class="line">        start_url = self.base_url + &apos;/xiaoshuodaquan/&apos;</div><div class="line">        yield Request(start_url, callback=self.get_novel_url)</div></pre></td></tr></table></figure></p>
<p>scrapy的Request()会把response自动返回callback的函数。<br>定义 get_novel_url<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># 获取小说url</div><div class="line">def get_novel_url(self, response):</div><div class="line">    base_a = response.xpath(&apos;//div[@class=&quot;novellist&quot;]/ul/li/a/@href&apos;).extract()</div><div class="line">    for a in base_a:</div><div class="line">        novel_a = self.base_url + a</div><div class="line">        yield Request(novel_a, callback=self.get_information_and_chapter, meta=&#123;&quot;novel_a&quot;: novel_a&#125;)</div></pre></td></tr></table></figure></p>
<p>可以使用meta参数进行传值，把meta中的结果传到，回掉函数的response中，使用response.meta[‘name’]取值。</p>
<p>接下来获取小说的基本信息。并存在item中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"># 获取基本信息</div><div class="line">def get_information_and_chapter(self, response):</div><div class="line">    item = BiqugeItem()</div><div class="line">    item[&apos;content&apos;] = &apos;&apos;.join(response.xpath(&apos;//meta[@property=&quot;og:description&quot;]/@content&apos;).extract()). \</div><div class="line">        replace(&apos; &apos;, &apos;&apos;). \</div><div class="line">        replace(&apos;\n&apos;, &apos;&apos;)</div><div class="line"></div><div class="line">    # 保存小说链接</div><div class="line">    novel_url = response.meta[&apos;novel_a&apos;]</div><div class="line">    item[&apos;url&apos;] = novel_url</div><div class="line"></div><div class="line">    # 提取小说名字</div><div class="line">    novel_name = &apos;&apos;.join(response.xpath(&apos;//meta[@property=&quot;og:novel:book_name&quot;]/@content&apos;).extract())</div><div class="line">    item[&apos;name&apos;] = novel_name</div><div class="line"></div><div class="line">    # 提取小说作者</div><div class="line">    item[&apos;author&apos;] = &apos;&apos;.join(response.xpath(&apos;//meta[@property=&quot;og:novel:author&quot;]/@content&apos;).extract())</div><div class="line"></div><div class="line">    # 从url中提取小说id</div><div class="line">    novel_id = &apos;&apos;.join(re.findall(&apos;\d&apos;, novel_url))</div><div class="line">    item[&apos;novel_id&apos;] = novel_id</div><div class="line">    yield item</div></pre></td></tr></table></figure></p>
<p>我使用mysql来进行存取数据，先建一张表，存小说的基本信息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE `novel`.`biquge_information` ( `id` INT NOT NULL AUTO_INCREMENT , `novel_id` VARCHAR(255) NOT NULL , `author` VARCHAR(255) NOT NULL , `url` VARCHAR(255) NOT NULL , `novel_name` VARCHAR(255) NOT NULL , `content` VARCHAR(255) NOT NULL , PRIMARY KEY (`id`)) ENGINE = InnoDB;</div></pre></td></tr></table></figure>
<p>在目录下新建一个包mysqlpipelines 在下面新建 <strong>init</strong>.py pipelines.py  sql.py<br><img src="/imgs/2017115/1.png" alt="结构"></p>
<p>需要安装mysql-connector<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install mysql-connector</div></pre></td></tr></table></figure></p>
<p>在setting.py中设置mysql的基本信息。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">MYSQL_HOST = &apos;127.0.0.1&apos;</div><div class="line">MYSQL_USER = &apos;root&apos;</div><div class="line">MYSQL_PASSWORD = &apos;&apos;</div><div class="line">MYSQL_PORT = &apos;3306&apos;</div><div class="line">MYSQL_DB = &apos;novel&apos;</div></pre></td></tr></table></figure></p>
<p>初始一个mysql游标，并且定义两个方法，其中一个用于插入数据，另外一个用于判断数据是否存在。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">import mysql.connector</div><div class="line">from biquge import settings</div><div class="line"></div><div class="line">cnx = mysql.connector.connect(user=settings.MYSQL_USER, password=settings.MYSQL_PASSWORD,</div><div class="line">                              host=settings.MYSQL_HOST, database=settings.MYSQL_DB)</div><div class="line">cur = cnx.cursor(buffered=True)</div><div class="line"></div><div class="line"></div><div class="line">class Sql:</div><div class="line">    @classmethod</div><div class="line">    def insert_infor(cls, novel_id, author, name, url, content):</div><div class="line">        sql = &apos;INSERT INTO `biquge_information` (`novel_id`, `novel_name`, `author`, `url`, `content`) &apos; \</div><div class="line">              &apos;VALUES (%(novel_id)s, %(novel_name)s, %(author)s, %(url)s, %(content)s)&apos;</div><div class="line">        value = &#123;</div><div class="line">            &apos;novel_id&apos;: novel_id,</div><div class="line">            &apos;novel_name&apos;: name,</div><div class="line">            &apos;author&apos;: author,</div><div class="line">            &apos;url&apos;: url,</div><div class="line">            &apos;content&apos;: content</div><div class="line">        &#125;</div><div class="line">        cur.execute(sql, value)</div><div class="line">        cnx.commit()</div><div class="line"></div><div class="line">    @classmethod</div><div class="line">    def select_novel_id(cls, novel_id):</div><div class="line">        sql = &apos;SELECT EXISTS(SELECT 1 FROM biquge_information WHERE novel_id=%(novel_id)s)&apos;</div><div class="line">        value = &#123;</div><div class="line">            &apos;novel_id&apos;: novel_id</div><div class="line">        &#125;</div><div class="line">        cur.execute(sql, value)</div><div class="line">        return cur.fetchall()[0]</div></pre></td></tr></table></figure>
<p>编写pipelines.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">from .sql import Sql</div><div class="line">from biquge.items import BiqugeItem</div><div class="line">from biquge.items import ChapterContentItem</div><div class="line"></div><div class="line"></div><div class="line">class BiqugePipeline(object):</div><div class="line"></div><div class="line">    def process_item(self, item, spider):</div><div class="line"></div><div class="line">        if isinstance(item, BiqugeItem):</div><div class="line">            novel_id = item[&apos;novel_id&apos;]</div><div class="line">            ret = Sql.select_novel_id(novel_id)</div><div class="line">            if ret[0] == 1:</div><div class="line">                print(&apos;小说已经存在&apos;)</div><div class="line">                pass</div><div class="line">            else:</div><div class="line">                name = item[&apos;name&apos;]</div><div class="line">                author = item[&apos;author&apos;]</div><div class="line">                content = item[&apos;content&apos;]</div><div class="line">                url = item[&apos;url&apos;]</div><div class="line">                print(&apos;小说不存在&apos;)</div><div class="line">                Sql.insert_infor(novel_id, author, name, url, content)</div></pre></td></tr></table></figure>
<p>运行start.py，爬虫就可以抓取笔趣阁上所有小说的基本信息了。<br>当然也可以抓取小说章节内容，可是懒癌犯了，就不写了，详见<a href="https://github.com/lylllcc/biquge.git" target="_blank" rel="external">完整项目</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前也没有认真写过爬虫项目，这应该算我第一个比较正式的爬虫项目。主要的目的就是练习一下&lt;a href=&quot;https://scrapy.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;scrapy&lt;/a&gt;框架。一个高效强大的爬虫框架。文档在&lt;a hr
    
    </summary>
    
    
      <category term="python" scheme="https://lylllcc.github.io/tags/python/"/>
    
      <category term="爬虫" scheme="https://lylllcc.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="scrapy" scheme="https://lylllcc.github.io/tags/scrapy/"/>
    
  </entry>
  
</feed>
